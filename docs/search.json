[
  {
    "objectID": "2024_chip_workshop.html",
    "href": "2024_chip_workshop.html",
    "title": "Analysing High Throughput Sequencing Data",
    "section": "",
    "text": "This is the homepage for the ChIP-Seq analysis course for biologists at the University of Edinburgh.\n\n\n\nThis workshop is designed as an introduction to ChIP-seq data analysis and will guide you through the following steps:\n\nProcessing data\n\nAssessing raw sequence data and performing quality control\nAligning sequence reads to a reference genome\nFiltering alignments for further analysis\nAssessing the quality of your ChIP-seq experiment\nVisualising alignments on a genome browser\n\n\n\nAnalysis\n\nSummarising ChIP-seq profiles across genes\nPeak calling\nMotif discovery\n\nThere is no such thing as a default pipeline. Although we mostly use standard parameters in this tutorial we hope to make you aware of the considerations you should take at each step. Make sure you understand your data and where it has come from. Use the correct tools for your dataset and read the tool documentation to see how different parameters affect your output!\n\nFor more information contact Shaun Webb."
  },
  {
    "objectID": "ChIP-seq_workshop_analysis.html#section-1",
    "href": "ChIP-seq_workshop_analysis.html#section-1",
    "title": "Analysis",
    "section": "",
    "text": "Align sequence reads to a reference genome\nAssess the quality of your ChIP-seq experiment\nVisualise alignments on a genome browser\nSummarise ChIP-seq profiles across genes\nPeak calling\nMotif discovery\nCreate and run shell scripts"
  },
  {
    "objectID": "ChIP-seq_workshop_processing.html#getting-started",
    "href": "ChIP-seq_workshop_processing.html#getting-started",
    "title": "Processing Data",
    "section": "1. Getting Started",
    "text": "1. Getting Started\nWe are using the Linux command line to run most of the tools we use today. If you are new to Linux please complete the Intro to Command Line Workshop.\n\n\nLogging in\nWe have several servers that you can login to. For the purpose of this practical we will use bifx-core2. No matter where you login, you will have access to the same files and programs. bifx-core3 is also available, both bifx-core2 and bifx-core3 require you to use a VPN.\nThere are several options to login to our machines. You can use the Terminal app on a Mac or equivalant Command Prompt or MobaXTerm on Windows. Login via X2G0 if you want a graphical interface.\nTo login via command line: ssh USER@bifx-core2.bio.ed.ac.uk\nLogin with your user credentials (the same as EASE)\nIf you are using MobaXTerm, an alternative way of logging in to the server is shown in the MobaXTerm demo.\nOnce you have typed in your password, you should see some welcome text and a prompt that looks something like this:\n[USERNAME]@bifx-core2:~$\n\n\n\n\nCreating A Web Directory\nIn order to view files created on the server, we need to create a public_html directory.\nAfter logging in you should be in your $HOME directory, check with;\npwd\nThis should show the PATH of your present working directory, which should now be your home directory as you have just logged in. You can return to this place at any time using the change directory command.\ncd\nYou have permissions to create files and directories under your home folder. Lets create some now which we will use later on.\nmkdir ~/public_html\nmkdir ~/public_html/TMP\nHere we have used the absolute path name for each directory using ~/ as a shortcut for your $HOME directory. Nested directories are separated by the forward slash ‘/’ sign.\nAs you have created ~/public_html, contents of this directory are available online with any web browser\nTo see it enter the following URL, changing yourUserName to what ever your username is.\nhttp://bifx-core3.bio.ed.ac.uk/~yourUserName\nFor new users this will be; https://bifx-core3.bio.ed.ac.uk/Public/yourUserName\n\n\n\nChIP-seq sequencing data\nThe datasets used in this exercise are derived from a single end ChIP-seq experiment (actually ChIP-exo) in S.cerevisiae. There are 2 biological replicates (though we recommend using 3 or more!) Reb1_R1 and Reb1_R2 as well as their corresponding input controls Input_R1 and Input_R2. For this experiment immunoprecipitation was performed with antibodies against Reb1. Reb1 recognizes a specific sequence (TTACCCG) and is involved in many aspects of transcriptional regulation by all three yeast RNA polymerases and promotes formation of nucleosome-free regions (NFRs). You can find the original publication here. For the purpose of this workshop we have randomly subsampled and filtered out poor quality reads to speed up runtime.\n\n\n\nDataset\nDescription\n\n\n\n\nReb1_R1\nChIP experiment, replicate 1\n\n\nReb1_R2\nChIP experiment, replicate 2\n\n\nInput_R1\nInput DNA, replicate 1\n\n\nInput_R2\nInput DNA, replicate 2\n\n\n\n\n\n\nObtaining data\nFirst, make a new directory for this tutorial and move into that directory. Then link the directory to your public html folder as we are going to make everything public in this tutorial.\ncd \nmkdir ChIP-seq_workshop\ncd ChIP-seq_workshop\nln -s $PWD ~/public_html/\nNext, create a subfolder called fastq for all of our sequence files and link the raw datasets to this folder:\nmkdir fastq\ncp /homes/library/training/ChIP-seq_workshop/data/*fq.gz fastq/.\nWhen you receive data from a sequencing centre the file should also be provided with an alphanumeric string known as an md5 checksum. We can think of this as a files passport or fingerprint and use it to verify our data and ensure it wasn’t corrupted or truncated during download. The md5 checksums for these files are below. Lets check that now using the md5sum command:\n\n\n\nmd5 checksum\nfilename\n\n\n\n\n914b4dda687a76b0d50e545e3ce705d6\nInput_R1.fq.gz\n\n\nf421ed18b71a801b236612cdde49dbaf\nInput_R2.fq.gz\n\n\ndd363301ad237ecb6c81c59ae97995a2\nReb1_R1.fq.gz\n\n\n06623f9e556876dd6c4d1dfdc4348698\nReb1_R2.fq.gz\n\n\n\ncd fastq #Move into the fastq directory\nmd5sum *.fq.gz > md5\ncat md5 #prints out the contents of md5\n#To check the files and md5 sums match at any time\nmd5sum -c md5 \n\n\n\nIntegrative Genomics Viewer\nLater we will use the Integrative Genomics Viewer (IGV), please install this on your own machine, alternatively you can use the App.\n\n\nKey Points:\n\n\nLog in to the bifx servers\nCreate a personal web directory\nCreate a project directory for fastq files\nRunning IGV"
  },
  {
    "objectID": "ChIP-seq_workshop_processing.html#section",
    "href": "ChIP-seq_workshop_processing.html#section",
    "title": "Processing Data",
    "section": "",
    "text": "Align reads to a reference genome\nUnderstand alignment file formats\nFilter alignments for further analyses"
  },
  {
    "objectID": "ChIP-seq_workshop_processing.html#quality-control",
    "href": "ChIP-seq_workshop_processing.html#quality-control",
    "title": "Processing Data",
    "section": "2. Quality Control",
    "text": "2. Quality Control\n\nFastQ files\nSequencing data will typically be provided to you in fastq format (.fq or .fastq) or as a compressed gzipped fastq (.fq.gz) in order to save space. We can view a gzipped file with the zless command, let’s take a look:\ncd ChIP-seq_workshop/fastq # Move into the fastq directory (if not already)\nzless Input_R1.fq.gz | head -n 12\nFastq files contain 4 lines per sequenced read:\n\nLine 1 begins with an ‘@’ character and is followed by a sequence identifier and an optional description\nLine 2 is the raw sequence\nLine 3 begins with a ‘+’ character and is optionally followed by the same sequence identifier\nLine 4 encodes the Phred quality score for the sequence in Line 2 as ASCII characters\n\n\nFirst we want to assess the quality of our sequencing data and check for any biases and contamination.\n\n\n\nFastQ screen\nWhen running a sequencing pipeline it is useful to know that your sequencing runs contain the types of sequence they’re supposed to. FastQ Screen allows you to set up a standard set of libraries against which all of your sequences can be searched. Your search libraries might contain the genomes of all of the organisms you work on, along with PhiX, Vectors or other contaminants commonly seen in sequencing experiments. We will run a screen of our sequences against human, mouse, rat, e.coli and s.cerevisiae (defaults):\ncd .. #Move up a directory again\nfastq_screen --conf /homes/genomes/tool_configs/fastq_screen/fastq_screen.conf fastq/*fq.gz --outdir fastq\n# * is a wild card character\nOnce complete take a look at the output images in your browser via your public html folder. This shows that most of your reads align to the yeast genome and that no reads align uniquely to other organisms:  \n\n\n\nFastQC\nFastQC provides simple quality control checks on raw sequence data coming from high throughput sequencing pipelines. It provides a modular set of analyses which you can use to get a quick impression of whether your data has any problems of which you should be aware before proceeding.\nfastqc fastq/*.fq.gz\nFastQC will create report files for each of your datasets which we can view in the browser. We will go through each of the images during the workshop. For future reference, specific guidance on how to interpret the output of each module is provided in the fastqc help pages.\nAn example of poor quality sequencing at the end of short reads:\n\nThe software gives a pass, fail or warning flag for each test based on what we would expect from a regular DNA-sequencing run. It is important to realise that FastQC does not understand the origin of your data and that different datasets will have different characteristics. For instance RNA sequencing often involves the use of random hexamer primers that are not as random as you might expect. The profile below in the first ~15 bases is perfectly normal for these samples but will be flagged as an error by FastQC:\n\nBisulfite treatment (used to investigate DNA methylation) converts most Cs in the genome to Ts, as we can see below. FastQC will not be happy with this profile, the point is, understand what you have sequenced and what you expect to see rather than blindly trusting the FastQC flag system!\n\nVisit the QCFail website for more examples and advice on quality control for NGS datasets.\n\n\n\nMultiQC\nWe can view summaries of multiple reports at once by using multiqc:\nmultiqc -o fastq fastq\nMultiQC searches for report files in a directory and compiles them into a single report. Open the multiqc report via a web browser to see how the raw datasets compare. Here we have the output of FastQ_screen and FastQC, but MultiQC works with the outputs of many tools other tools which we’ll see later.\nIf we look at the adapter content and over represented sequences sections we can see a small amount of contamination particularly in the second replicates.\n\n\n\nParallelisation\nUp until now we have run command line tools on each one of our datasets in serial, this means they run one after the other. In this tutorial we only have a few small datasets and the tools run relatively quickly, but this approach won’t scale well to multiple large datasets. A more efficient approach is to run all of our datasets in parallel, later we will create a script.\n\n\n\nparallel\nUnix has a program called parallel which allows you to run tools on multiple datasets at the same time. The following command would list all of your gzipped fastq files and pipe them into parallel.\nls fastq/*fq.gz | parallel -j 4 fastqc {} &\nps f\n\nls lists files ending with .fq.gz in your fastq directory and pipes the names in to parallel\nThe parallel -j flag stands for juggle and tells parallel to run 4 processes at the same time.\nIn this case we are running fastqc and the {} is a place holder for the filenames we are piping in.\nThe & character runs these jobs in the background so we can continue to use the terminal.\nps is the process status tool which shows jobs running in the current session, we should see 4 instances of fastqc running.\n\n\nFirst, let’s create a file that lists our sample names so we can feed this into parallel. We could just type this manually, but here fastq files are ‘piped’ into parallel as above but we use regular expression within ‘sed’ to remove the name ending, this can now be used to name all files.\nls fastq/*fq.gz | parallel basename | sed s/.fq.gz// > samples.txt\n\n\n\n Challenge:\n\nSee if you can adapt the FastQC command to use the samples.txt file in parallel.\n\n\nHint: cat samples.txt will print the names of the samples.\n\n\nSolution\n\n\nSolution. \n\n\n\ncat samples.txt | parallel -j 4 fastqc fastq/{}.fq.gz\n\n\n\n\n\n\nPre-processing: Quality trimming and adapter removal\nFrom the FastQC report we can see that the overall quality of our sequencing is good, however it is good practice to perform some pre-processing and filtering of reads. Poor quality sequencing can make a read less alignable so it is good practice to quality trim the ends of reads until we get to the high quality portion. Trimming is not always neccessary as some mapping programs will trim the reads for you or perform soft clipping where only part of a read is required to align but studies have shown that pre-processing generally improves alignment rate if done correctly.\nSequencing libraries are normally constructed by ligating adapters to fragments of DNA or RNA. If your read length is longer than your fragment then sequenced reads will contain the adapter sequence. Adapter removal is also a necessary consideration for your QC workflow, especially if adapters are detected by FastQC.\nAn example of adapter contamination at the end of reads: \nOnce reads have been trimmed they will vary in length. You may want to filter out reads that are now too short to be uniquely mapped. Normally a cutoff of 20-30bp is standard.\nTrim with caution and think about the consequences of having different length reads later on in your pipeline. In fact, it is possible to overtrim your reads and aggressively remove valid data.\n\n\n\nCutadapt\nCutadapt finds and removes unwanted sequences from your high-throughput sequencing reads. Cutadapt can perform quality trimming, adapter removal and read filtering as well as many other operations to prepare your reads for optimal alignment. We will run cutadapt with the following parameters:\n\n-a : The sequence of the adapter to remove\n-q : Trim reads from the 3’ end with the given quality threshold (Phred score)\n–minimum-length : Filter out reads below this length\n\ncat samples.txt | parallel -j 4 \"cutadapt -a AGATCGGAAGAG -q 20 --minimum-length 36 -o fastq/{}.trim.fq.gz fastq/{}.fq.gz > fastq/{}.trim.cutadapt_report.txt\"\nWe will also run FastQC on the trimmed dataset.\ncat samples.txt | parallel -j 4 fastqc fastq/{}.trim.fq.gz\nTo view a cutadapt report:\nless fastq/Reb1_R1.trim.cutadapt_report.txt\nhit ‘q’ to exit less.\nLet’s compare the fastqc reports using multiqc. As you have run it already you need to use the force (-f) flag to get it to overwrite the current report.\nmultiqc -f -o fastq fastq\nOpen the multiqc report via a web browser to see how the raw and trimmed datasets compare.\n\n\n\nOther QC software worth investigating\n\nTrimmomatic is very good and runs with Java\nTrim Galore!\n\n\n\nKey Aims:\n\n\nUnderstand the Fastq format\nCheck for contaminants\nAssess sequence quality\nUnderstand parallel\nTrim your data"
  },
  {
    "objectID": "ChIP-seq_workshop_processing.html#section-1",
    "href": "ChIP-seq_workshop_processing.html#section-1",
    "title": "Processing Data",
    "section": "",
    "text": "Align reads to a reference genome\nUnderstand alignment file formats\nFilter alignments for further analyses"
  },
  {
    "objectID": "ChIP-seq_workshop_processing.html#mapping-and-filtering",
    "href": "ChIP-seq_workshop_processing.html#mapping-and-filtering",
    "title": "Processing Data",
    "section": "3. Mapping and Filtering",
    "text": "3. Mapping and Filtering\n\nRead Alignment\nIn this workshop we are going to align our ChIP-seq reads to the yeast reference genome. There are many tools available for mapping reads each with their own purposes and strengths. We are going to use BWA as it is suitable for aligning single end short ChIP-seq reads and has a reasonable balance of accuracy and speed.\n\n\n\nGenome assemblies and indexing\nFirst, we need to select a reference genome to align to. Every time a reference genome is released or updated it is given a new name, often referred to as the genome build or assembly (..hg18, hg19, hg38). It is important to realise that different builds of the same genome are different sequences and thus their co-ordinate sytems are incompatable. For instance position 10000000 on chr1 is T in hg19 and G in hg38.\nWe are going to map our reads to the latest release of the yeast genome sacCer3. We need to create an index file from the sacCer3 sequence so that BWA can quickly access the reference sequences. Luckily many of these indexes are pre-computed on our servers and stored under the ~genomes directory so you would only need to run this step for a new genome:\n\n# do not run\n#bwa index -p /homes/genomes/s.cerevisiae/sacCer3/bwa_indexes/sacCer3 -a is /homes/genomes/s.cerevisiae/sacCer3/sacCer3.fa\n\n\n\n\nMapping reads with BWA\nOnce we have an index we can align our reads to the sacCer3 genome with BWA. This will take ~5 minutes to run so let’s get it started:\nFirst create a new directory for the alignments and sub directories for each sample to keep your data organsied:\nmkdir bwa_out\ncat samples.txt | parallel -j 4 mkdir bwa_out/{}\nThen run bwa on each of your samples. Note that we are redirecting the output to a file using > so we put the full command in quotes for parallel to execute.\ncat samples.txt | parallel -j 4 \"bwa mem -t 5 -a -R '@RG\\tID:{}\\tPL:ILLUMINA' -M /homes/genomes/s.cerevisiae/sacCer3/bwa_indexes/sacCer3 fastq/{}.trim.fq.gz > bwa_out/{}/{}.sam\"\n\nThis will take slightly longer to run. \n\n\n\nSAM/BAM/CRAM format and Samtools\nThe standard output for most mapping software is SAM (sequence alignment/map format). SAM files contain many columns that describe the position of each alignment as well as information on the quality of the alignment, mismatches, the number of times a read mapped, mapping of paired ends and other custom flags and statistics. SAM files can be very large so there are compressed alternatives BAM and CRAM. The samtools package has many useful tools for viewing and manipulating files in SAM format. We will use some of these below.\nTake a look at the SAM format specification and the first few lines of your SAM output using samtools:\nsamtools view bwa_out/Reb1_R1/Reb1_R1.sam | less \nThe second column is the SAM flag and contains coded information about each alignment. Use the Explain SAM flags resource to find out more about the alignments in your file.\nWe can also see the samtools header using the -h flag which contains information on the parameters and indexes used to create the file.\nsamtools view -h bwa_out/Reb1_R1/Reb1_R1.sam | less\nWe can use samtools to sort the sam file by co-ordinate and output in the binary format BAM to save disk space. The BAM file can also be indexed to allow quick programmatic access for visualisation and processing. We can feed multiple commands into our call to parallel by separating them with a semi colon ;.\ncat samples.txt | parallel -j 4 \"samtools sort bwa_out/{}/{}.sam -o bwa_out/{}/{}.bam -T bwa_out/{}/{} -O BAM; samtools index bwa_out/{}/{}.bam\"\nTake a look at the contents of the bwa_out directory now. The -lh flag prints out a directory in list view with human readable file sizes.\nls -lh bwa_out/*  \nNotice the difference in size between the SAM and BAM files and the .bai file which is the bam index. Let’s look at one of the BAM files using samtools idxstats to see where our reads align\nsamtools idxstats bwa_out/Reb1_R1/Reb1_R1.bam\nThe third column represents the number of alignments to each chromosome and at the bottom we can see some reads which have not mapped at all."
  },
  {
    "objectID": "ChIP-seq_workshop_processing.html#section-2",
    "href": "ChIP-seq_workshop_processing.html#section-2",
    "title": "Processing Data",
    "section": "",
    "text": "Align reads to a reference genome\nUnderstand alignment file formats\nFilter alignments for further analyses"
  },
  {
    "objectID": "ChIP-seq_workshop_processing.html#visualisation",
    "href": "ChIP-seq_workshop_processing.html#visualisation",
    "title": "Processing Data",
    "section": "5. Visualisation",
    "text": "5. Visualisation\n\nConverting BAM files to bigWig tracks\nVisually inspecting data via a genome browser is often the first step in any analysis. Has the sequencing worked as expected? Are there noticeable differences between my samples by eye? BAM files are typically large and contain detailed information for every read, thus they are slow to access and view on a genome browser. If we are only interested in read profiles we can convert our BAM files to graphs of sequencing depth per base. The wiggle file format has three columns to represent a genomic interval and a 4th as a score which will represent the number of reads overlapping that region. We will create a compressed version of this format called bigWig for use with genome browsers. To do this we are going to use the deepTools package which has a lot of useful tools particularly for ChIP-seq analysis.\nThe bamCoverage tool in deepTools has many different options for normalising, smoothing and filtering your BAM files before converting to bigWig and the documentation is worth a read.\ncat samples.txt | parallel -j 4 \"bamCoverage -bs 1 --normalizeUsing BPM -e 200 -b bwa_out/{}/{}.uniq.bam --outFileName bwa_out/{}/{}.uniq.bw\"\n\n-bs is the bin size. We set this to 1, but we could smooth our read coverage into larger bins across the genome.\n–normalizeUsing BPM is a normalisation strategy akin to TPM in RNA-seq and stands for “bins per million”. There are several strategies to choose or we can ignore normalisation.\n-e 200 extends our reads to 200b as this is the average fragment length. With paired end data the reads would be extended to match the ends of both reads but with single end we must supply a value.\n\n\n\n\nIntegrative Genomics Viewer (IGV)\nIGV (Integrative genomics viewer) is a powerful genome browser from the Broad institute. It is perfect for viewing your sequencing files as loading data is easy, common formats are understood, views are customisable and navigation is quick.\nFirst let’s put all of our visualisation files in one folder.\nmkdir visualisation\nln -s $PWD/bwa_out/*/*bw visualisation\nln -s $PWD/bwa_out/*/*uniq.bam* visualisation\nIt is best to download the desktop app for use on your own machine but if you are using a university managed computer you can access the web app at https://igv.org/app/.\nOpen IGV and set the genome to sacCer3, then find your visualisation folder online. In the desktop version you can drag and drop files into IGV. If you are using the webapp you will need to download the files you require and open them using the tracks menu.\nOpen a BAM file and a corresponding bigWig file into IGV. Make sure the .bai index file is in the same folder as the BAM.\n\nFirst, let’s navigate the chromosome:\n\nUse the + / - zoom bar at the top right\nDrag and highlight an area to zoom on the genome ruler\nType a position or gene identifier in to the search box\nIf we zoom in close enough our BAM file will load and we can investigate alignments of individual reads\n\nNow let’s customise our view:\nDESKTOP APP\n\nSelect Tracks->Fit Data To Window To automatically set track heights to fill the view\nRight click on the Refseq genes track and switch between display modes Collapsed, Expanded and Squished. (Use the horizontal panel divider to adjust height)\nUse the right click menu on the bigwig track name to customise the display. Try the following:\nRename the track\nChange the track colour\nChange the graph type\nChange the windowing function\nAdjust the data range\nSet the scale to Autoscale. This automatically sets the Y-axis to the height of the tallest peak in your view.\n\nWEB APP\n\nUse the cog on the Ensembl genes track to switch between display modes Collapsed, Expanded and Squished.\nCustomise the bigwig track by trying the following:\nRename the track\nChange the track colour\nChange the track height\nSet a minimum and maximum Y-axis\nSet the scale to Autoscale. This automatically sets the Y-axis to the height of the tallest peak in your view.\n\n\n\n\n Discussion:\n\nBAM vs bigWig\n\nWhy are the coverage profiles different?\nWhich format is faster to view?\nCan you identify mismatches in read alignment?\nWhat other information do the BAM files give us?\n\nNow let’s load all our bigwig files\n\nCan you identify any potential peaks in the ChIP samples\nDo you see any peaks that are also present in the Input?\n\n\n\n\n\n\nOther genome browsers\n\nEnsembl and UCSC websites have online genome browsers where you can view your data alongside published experimental data and genomic annotations in their databases.\n\n\n\n\nTidy Up!\nFiles are large, disk space is expensive, remove any unwanted or temporary files from your folder. We should always keep the raw data (fastq) and our final processed datasets (BAM, bigWig etc) and the script we used to generate them. SAM files are large and should be removed once converted to BAM.\nrm fastq/*trim*fq.gz #Remove trimmed fastq temp files\nrm bwa_out/*/*.sam #Remove sam files\n\n\n\nKey Aims:\n\n\nCreate tracks from our data\nVisualise alignments on a genome browser"
  },
  {
    "objectID": "ChIP-seq_workshop_processing.html#section-4",
    "href": "ChIP-seq_workshop_processing.html#section-4",
    "title": "Processing Data",
    "section": "",
    "text": "Create tracks from our data\nVisualise alignments on a genome browser"
  },
  {
    "objectID": "ChIP-seq_workshop_analysis.html",
    "href": "ChIP-seq_workshop_analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "Assess the quality of ChIP-Seq experiment\nCall peaks in data\nUnderstand scripting"
  },
  {
    "objectID": "ChIP-seq_workshop_analysis.html#assessing-chip-quality",
    "href": "ChIP-seq_workshop_analysis.html#assessing-chip-quality",
    "title": "Analysis",
    "section": "1. Assessing ChIP quality",
    "text": "1. Assessing ChIP quality\nAfter we have mapped and filtered the reads it is time to make some inferences about how good the underlying data is.\n\n\nCorrelation amongst samples\nIn our experiment there are two replicates, each containing treatment and input (control) datasets. The first thing we can check is if the samples are correlated (in other words if treatment and control samples across the two replicates contain this same kind of signal). To do this we first generate a read count matrix using deepTools multiBamSummary. It will take around 5 minutes to complete so it’s best to set it running and read on.\nmultiBamSummary bins -b bwa_out/*/*uniq.bam -bs 10000 -o cov.matrix -e 200 --smartLabels\nThis tool splits the genome into bins of fixed size (10,000 bp in our example) and computes the number of reads falling within each bin. The output is in a compressed unreadable format but here is a fragment of the data produced:\n#'chr' 'start' 'end'  'Reb1_R1'  'Input_R1'  'Input_R1'  'Reb1_R2'\nchrVI      0     10000   19.0         41.0         3.0        6.0\nchrVI   10000    20000   29.0         30.0        13.0        5.0\nchrVI   20000    30000    0.0          0.0         0.0        0.0\nchrVI   30000    40000    0.0          2.0         0.0        0.0\nchrVI   40000    50000 7447.0        139.0         7.0     2645.0\nWe can then feed this matrix into DeepTools plotCorrelation to generate a heat map.\nplotCorrelation -in cov.matrix -p heatmap -c spearman -o spearman_cor.png\n\nHere we can see that treatments (Reb1) and controls (input) correlate well with each other, while correlation between treatments and controls is weak. This is a good sign implying that there is reproducible signal on our data.\n\n\n\nAssessing signal strength\nHow do we tell if we have signal coming from ChIP enrichment? One way of doing this is Signal Extraction Scaling (SES) proposed by Diaz:2012. SES works as follows: Suppose we have two datasets: ChIP and Input DNA. We divide the genome into N non-overlapping windows and for each window we compute the number of reads in each sample. We then sort the windows by the number of reads in 1 sample and plot the cumulative fraction of reads. We expect Input samples to show approximately equal distribution of reads across all windows and thus give a flat line, whereas ChIP samples with very defined enrichments should have a high proportion of reads in a small number of bins. DeepTools provides a nice explanation of how the success of a ChIP experiment can be judged based on SES (also called fingerprint) plots:\n\nWe can apply this to our own data with DeepTools plotFingerprint:\n\n# very slow\nplotFingerprint -b bwa_out/*/*uniq.bam -e 200 --smartLabels -o fingerprint.png &\n\nThe fingerprint tool can take a long time to run so we will run in the background (&) and continue with the tutorial. However, You should get a plot that looks like the one below where approximately 30% of the ChIP reads are concentrated in to a small number of bins.\n\n\n\n\nSummarizing ChIP signal enrichment across genes\nHow many genes contain upstream regions enriched in Reb1 ChIP-seq reads? This is often represented as a heatmap, here is an example of histone modification ChIP-seq data from the Deeptools documentation:\n\nFirst we want to create normalised datasets of ChIP signal vs Input for our two replicates. This is done using DeepTools bamCompare and gives us a bigWig file of normalised log2 ratios.\nbamCompare -b1 bwa_out/Reb1_R1/Reb1_R1.uniq.bam -b2 bwa_out/Input_R1/Input_R1.uniq.bam -o Reb1_R1.norm.bw -e 200 --operation log2 --normalizeUsing BPM --scaleFactorsMethod None &\nbamCompare -b1 bwa_out/Reb1_R2/Reb1_R2.uniq.bam -b2 bwa_out/Input_R2/Input_R2.uniq.bam -o Reb1_R2.norm.bw -e 200 --operation log2 --normalizeUsing BPM --scaleFactorsMethod None &\nThe bamCompare tool should take ~5 minutes to complete so you can move on to the next step. These bigWig files can also be viewed in IGV so go ahead and link them to the visualisation folder:\nln -s $PWD/*norm.bw visualisation\nBecause we want to plot enrichment around genes we need the gene annotations. These are already downloaded in the ~genomes folder so let’s link them to our current directory.\ncp /homes/genomes/s.cerevisiae/sacCer3/annotation/UCSC_sgdGene.bed .\nTake a look at this file:\nhead UCSC_sgdGene.bed\nThese gene annotations are downloaded from the UCSC website in bed12 format: The co-ordinates are in columns 1-3, the gene name in column 4 and the strand in column 6.\nTo prepare data necessary for drawing the heatmap we will use deepTools computeMatrix. Here we supply our normalised datasets as (S)core files along with the yeast genes from UCSC as the (R)egions to plot. We are going to compute reads around the TSS in 100b bins, with a 2kb flanking region either side.\ncomputeMatrix reference-point -R UCSC_sgdGene.bed -S Reb1_R1.norm.bw Reb1_R2.norm.bw -o TSS.matrix --referencePoint TSS --upstream 2000 --downstream 2000 -bs 100 --smartLabels\nThe warning “Skipping Q0010, due to being absent in the computeMatrix output.” refers to mitochondrial genes and chrM has been exlcuded.\nFinally, we can visualize the heatmap by using the DeepTools plotHeatmap tool:\nplotHeatmap -m TSS.matrix -o TSS.heatmap.png \nThe resulting image shows that a significant fraction of genes have peaks of Reb1 enrichment within their upstream regions:\n\n\nReveal Plot\n\n\nSolution. \n\n\n\n\n\n\nIt is worth taking a look at the documentation for the deepTools package in your own time to understand the range of tools and how different parameters affect your data and the different plots you can make."
  },
  {
    "objectID": "ChIP-seq_workshop_analysis.html#section",
    "href": "ChIP-seq_workshop_analysis.html#section",
    "title": "Analysis",
    "section": "",
    "text": "Align sequence reads to a reference genome\nAssess the quality of your ChIP-seq experiment\nVisualise alignments on a genome browser\nSummarise ChIP-seq profiles across genes\nPeak calling\nMotif discovery\nCreate and run shell scripts"
  },
  {
    "objectID": "ChIP-seq_workshop_processing.html#section-3",
    "href": "ChIP-seq_workshop_processing.html#section-3",
    "title": "Processing Data",
    "section": "",
    "text": "Create tracks from our data\nVisualise alignments on a genome browser"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysing High Throughput Sequencing Data",
    "section": "",
    "text": "This is the homepage for the ChIP-Seq analysis course for biologists at the University of Edinburgh.\n\n\n\nThis workshop is designed as an introduction to ChIP-seq data analysis and will guide you through the following steps:\n\nProcessing data\n\nAssessing raw sequence data and performing quality control\nAligning sequence reads to a reference genome\nFiltering alignments for further analysis\nAssessing the quality of your ChIP-seq experiment\nVisualising alignments on a genome browser\n\n\n\nAnalysis\n\nSummarising ChIP-seq profiles across genes\nPeak calling\nMotif discovery\n\nThere is no such thing as a default pipeline. Although we mostly use standard parameters in this tutorial we hope to make you aware of the considerations you should take at each step. Make sure you understand your data and where it has come from. Use the correct tools for your dataset and read the tool documentation to see how different parameters affect your output!\n\nFor more information contact Shaun Webb."
  },
  {
    "objectID": "ChIP-seq_workshop_processing.html",
    "href": "ChIP-seq_workshop_processing.html",
    "title": "Processing Data",
    "section": "",
    "text": "Set up a project folder\nAssess quality of sequence data\nAlign reads to a reference genome and perform post alignment filtering\nView read coverage profiles and alignments in a genome browser"
  },
  {
    "objectID": "ChIP-seq_workshop_analysis.html#calling-peaks",
    "href": "ChIP-seq_workshop_analysis.html#calling-peaks",
    "title": "Analysis",
    "section": "2. Calling peaks",
    "text": "2. Calling peaks\nWhile the peaks we have found in the genome browser are pretty clear and consistent across the two replicates, looking at the entire genome in the browser is hardly an efficient way to identify all peaks of Reb1 binding. There are several ways to identify genome-wide binding events and three of these methods are summarised below:\n\n\n\nMACS2\nIn this tutorial we will use the program MACS2. MACS2 performs several steps for calling peaks from paired treatment/control datasets:\n\nHere is a concise description of these steps:\n\nRemoving redundancy - MACS retains uniquely mapped reads and removes reads that are repeatedly mapped to the same location. This reduces effects of PCR amplification biases during library preparation.\nBuild model and estimate fragment size - one of the MACS inputs is the fragment size or bandwidth, which is approximate size of DNA fragments generated during fragmentation step of library preparation. MACS first slides a window sized at twice the bandwidth across the genome and finds instances where read counts enriched by between 10 and 30 fold relative to the genome background. It then randomly samples 1,000 of such regions and builds the model. To build the model it separates reads mapping to each of the strands and builds two distributions (two modes). The midpoint between the two modes is the middle of the binding size and the distance between the modes is the fragment size d (see Figure below).\nGenerate peaks - now that d has been defined MACS slides a window of size 2d across the genome to identify regions significantly enriched in the ChIP sample. MACS assumes that background reads obey Poisson distribution. Thus given the number of reads in a given interval within the control sample we can calculate the probability of having observed number of reads in the ChIP sample (e.g., see flood example here). This procedure is performed for several intervals around the examined location (2d, 1kb, 5kb, 10kb, and the whole genome) and the maximum value is chosen. One problem with this approach is that it only works if both samples (ChIP and control) are sequenced to the same depth, which is not usually happening in practice. To correct this MACS scales down the larger sample.\nCompute False Discovery Rate (FDR) - Feng:2012 explains computing FDR in MACS as follows: “When a control sample is available (and you should really always use it), MACS can also estimate an empirical FDR for every peak by exchanging the ChIP-seq and control samples and identifying peaks in the control sample using the same set of parameters used for the ChIP-seq sample. Because the control sample should not exhibit read enrichment, any such peaks found by MACS can be regarded as false positives. For a particular P value threshold, the empirical FDR is then calculated as the number of control peaks passing the threshold divided by the number of ChIP-seq peaks passing the same threshold.”\n\n\nPeaks mapped to two strands are treated separately to build two coverage density profiles - two modes. The distance between the modes is the fragment size d.\n\n\n\nFinding peaks with MACS\nIn our case we have two replicates each containing ChIP and input DNA samples. We will run MACS2 by pooling data (combining two ChIP samples and two inputs, respectively) but a more stringent approach might be to run each replicate separately and look for overlapping peaks.\nmacs2 callpeak -t bwa_out/Reb1_R1/Reb1_R1.uniq.bam bwa_out/Reb1_R2/Reb1_R2.uniq.bam -c bwa_out/Input_R1/Input_R1.uniq.bam bwa_out/Input_R2/Input_R2.uniq.bam -g 12000000 --nomodel -n Reb1\nThe -g flag is the approximate genome size for s.cerevisiae. In this example MACS does not have enough reads to build a shifting model with our reduced number of reads so we skip this step by using –nomodel. This should be removed when analysing a complete dataset.\nMACS2 will produce three outputs, a peaks.narrowPeak file containing the co-ordinates of peaks, a peaks.xls file of peaks for excel and a summits.bed file with the location of the greatest enrichment within each peak.\n\n\n\nInspecting Peaks\nLooking at MACS2 data we have 816 peaks. Take a look at the peak calling output, the columns are as follows:\n\nChromosome\nStart\nEnd\nIterative id given by MACS2\nInteger score for display\nStrand (irrelevant in this case)\nFold-change (fold enrichment for this peak summit against random Poisson distribution with local lambda)\nlog10 P-value (e.g., 17.68 is 1 x 10-17)\nlog10 Q-value from Benjamini–Hochberg–Yekutieli procedure\nRelative summit position to peak start\n\nYou can load the peak files into IGV and see how they correspond to our bigwig tracks.\n\n\n\n\nWhat sequence motifs are found within peaks?\nIn this experiment antibodies against Reb1 protein have been used for immunoprecipitaion. The recognition site for Reb1 is TTACCCG (Badis:2008). To find out which sequence motifs are found within our peaks we first need to convert coordinates into underlying sequences. The bedTools suite has a tool to do this.\nbedtools getfasta -fi /homes/genomes/s.cerevisiae/sacCer3/sacCer3.fa -bed Reb1_peaks.narrowPeak -fo Reb1_peaks.fasta &\nNow we can run MEME to discover sequence motifs within our data. The meme-chip tool is designed specifically for looking for motifs in ChIP-seq data:\nmeme-chip Reb1_peaks.fasta\nWhen complete, MEME will create a new folder called memechip_out in your directory and an html (web page) report. See if you can open this in your web browser and find the TTACCCG Reb1 binding motif:"
  },
  {
    "objectID": "ChIP-seq_workshop_analysis.html#scripting",
    "href": "ChIP-seq_workshop_analysis.html#scripting",
    "title": "Analysis",
    "section": "3. Scripting",
    "text": "3. Scripting\nPutting all of your commands into a script is good practice for keeping track of your analyses and for reproducibility. We want to write scripts so they can be used again on different datasets and avoid hardcoding the names of files.\nIn the first data processing section we created our alignments (BAM) and visualisation files (bigWig) and this is normally a branching point for downstream analyses that quantify and annotate the data.\nHere is a pipeline containing everything up to that point; pipeline.sh\nLooking at this pipeline script you’ll notice we are not running parallel every time we run a command, instead we launch the script using parallel and the sample names are passed using the special ‘$1’ bash variable. This is more efficient as the samples will run through each command independently.\nUsing this we can re-run everything from start to end in one go.\nmkdir CS_workshop_tmp #Create a temporary directory\ncd CS_workshop_tmp #Move into that directory\ncp /homes/library/training/ChIP-seq_workshop/pipeline.sh . # Copy the pipeline into the new directory\ncp ../samples.txt . #Copy the samples file\nnohup cat samples.txt | parallel -j 4 bash pipeline.sh {} > pipeline.log & #Run the shell script (See Below) \ncd .. #Move back to the main directory\n\nThe nohup command stands for no hangup and keeps the process running even if you log out of your command line session.\nWe use the tool bash and the name of our script to run our commands through the shell (‘sh’ will also work).\nWe then redirect > the output to a log file to keep track of any errors.\nWe use the & to run everything in the background to continue using the terminal.\n\nYou can keep track of your pipeline by using ps or looking at the log file.\nps f\ntail CS_workshop_tmp/pipeline.log # Shows the end of a file\nNote any commands run on multiple samples will need be to run separately (after pipeline finished)\nmultiqc CS_workshop_tmp/fastq -o CS_workshop_tmp/fastq\n\nDon’t forget to tidy up.\nrm CS_workshop_tmp/fastq/*trim*fq.gz #Remove trimmed fastq temp files\nrm CS_workshop_tmp/bwa_out/*/*.sam #Remove sam files\n\nFor reference, here is a script containing all the steps run in the analysis section; analysis/sh\n\nAlthough shell scripts save all of your commands they do not necessarily track the versions of software that you use or the current state of your environment and other tool dependencies. More advanced methods of pipelining and containerisation are recommended for fully reproducible analysis and sustainable programming.\nNow that we have completed the workshop you should be able to do the following:\n\n\nSummary:\n\n\nAssess the quality of your experiment\nSummarise ChIP-seq profiles across genes\nPeak calling\nMotif discovery\nCreate and run shell scripts\n\n\nUse these links to download the full pipeline.sh and analysis.sh scripts.\nIf you are interested in other ways to plot or interrogate your data, the R programming language has many pre-built libraries to specifically analyse genomic data. Several of these are summarised in our genomic regions of interest workshop including gene annotation and plotting ChIP-seq signals. There are also packages for differential binding and statistical analysis. R is a very powerful tool for working with genomic data, applying statistics and creating visualisations. I would highly recommend learning R if you want to perform further downstream analyses."
  },
  {
    "objectID": "ChIP-seq_workshop_processing.html#post-processing",
    "href": "ChIP-seq_workshop_processing.html#post-processing",
    "title": "Processing Data",
    "section": "4. Post processing",
    "text": "4. Post processing\n\n\nFiltering reads with samtools\nNow that we have aligned our reads we may want to do some filtering before any downstream analysis. Make sure you are aware of the alignments that are reported by your mapping program and the parameters used. For instance, are unmapped reads reported? Are all alignments to repeats reported or just one? Are paired-end alignments still reported if only one end maps?\nThere are many ways to filter your BAM files with samtools and other programs to remove unwanted alignments that may negatively affect your downstream analysis. This will not be covered in depth here, instead we will simply remove all non-uniquely mapped reads. These reads map to multiple regions of the genome and can skew classification of peaks in our data. In this case we are not interested in looking at repeat regions of the genome so we will remove these reads. This can be done by filtering out all reads with mapping quality less than 20.\n\n\n\nMultimap reads and Duplicate reads\nMultimap and duplicate reads are often confused so it is important to understand what these are and how they affect your data:\n\nMultimap reads = The read exists once in your library and aligns to multiple repeat locations in the reference genome\nDuplicate reads = Multiple reads with the same sequence align to identical locations in the genome.\n\n\nMultimap reads are difficult to analyse as their ambiguity can confound results. Many applications require the use of unique alignments only, thus multimap reads need to be removed from your BAM file. Aligners assign a mapping quality to each read (column 5 in BAM) between 0 and 255 that describes its confidence in the alignment position. Assigned mapping qualities differ between mappers and BWA uses a phred score to measure the accuracy of an alignment. Filtering out reads with a mapping quality < 20 means that all remaining alignment positions are 99% accurate. We can use samtools view -q to filter based on mapping quality\ncat samples.txt | parallel -j 4 \"samtools view -b -q 20 bwa_out/{}/{}.bam -o bwa_out/{}/{}.uniq.bam; samtools index bwa_out/{}/{}.uniq.bam\"\nNote that there are aligners and analysis packages that attempt to deal with multimap reads and assign weights to each alignment, although methods are still in development. If you are interested in repeat elements or don’t want to discard any potentially useful information then you will need a strategy to deal with these reads.\nDuplicate reads are often observed as tall spikes in your read depth profile where reads are stacked directly on top of each other. A high level of duplication in your library is often a sign of over amplification by PCR and we may want to remove this bias from our result. However, these reads may also derive from separate fragments of DNA in your sample, thus we would be removing real data. It is often a good idea to mark your duplicate reads and produce outputs both with and without duplicates for comparison. Read more about duplication bias here.\nThe Picard package has many useful utilities for manipulating SAM/BAM files. The MarkDuplicates tool will check the alignment positions for duplicate reads and mark or remove them from your data depending on how you wish to treat them.\nUsing paired-end reads or random primers in your library preparation can help separate some of the original reads from PCR duplicates.\n\n\n\nGenome blacklists\nThe ENCODE project produced 100s of NGS datasets and found that certain regions of the genome were consistently prone to overinflated read depths regardless of the sample or preparation. Some of these are repeats of variable copy number, others are likely to be similar to repeat regions in unsequenced portions of the genome (telomeres, centromeres, satellites etc.). These are typically seen as large towers of reads that dominate your read profiles. It is probably a good idea to remove these regions from downstream analyses or remove the reads that align all together. ENCODE subsequently released genome blacklists for human and mouse, for other species you can identify these regions by eye.\nBedTools is an extremely useful tool suite for performing operations on genomic intervals and alignments and comparing multiple datasets. The intersect tool can find overlaps between reads in a BAM file -abam and blacklist regions in a bed file -b and output all the reads that DON’T intersect -v.\n\nIn some cases you may also want to remove ribosomal RNA/DNA reads that make up a bulk of your sample.\n\n\n\nHow many reads?\nAs we used the -a flag when aligning bwa outputs multiple alignments per read if they exist, meaning a read may have many entries within a BAM file. However, bwa will assign one of the best hits as a primary alignment and all the others as secondary. Unmapped reads are also printed to the output file and these two factors mean that the total number of lines does not correspond to the total number of mapped reads.\nsamtools view can tell us how many lines are in a bam file using the -c flag to count. We can filter alignments using -f (keep) or -F (discard) on the SAM flag column.\nsamtools view -c bwa_out/Reb1_R1/Reb1_R1.bam\n5444557\nsamtools view -c -f 4 bwa_out/Reb1_R1/Reb1_R1.bam \n# counts number of unmapped reads (flag = '4')\n144915\n\n\n\n Challenge:\n\n\nHow many mapped reads do you have?\nHow many of these align to a single location on the genome?\nHow many have a MAPQ score above 20?\n\n\n\nSolution\n\n\nSolution. \n\n\n\nsamtools view -c -F 4 bwa_out/Reb1_R1/Reb1_R1.bam \n# -F 4 filters out unmapped reads\nReb1_R1 has 5299642 mapped reads\nsamtools view -c -F 260 bwa_out/Reb1_R1/Reb1_R1.bam\n# -F 260 filters out unmapped reads (4) and non primary alignments (256)\n2572126 reads map to a single ‘unique’ location\nsamtools view -c -q 20 bwa_out/Reb1_R1/Reb1_R1.bam\n1967026 reads have a MAPQ > 20\nNote our filtered uniq.bams have excluded unmapped/non primary and MAPQ < 20 all in one.\nsamtools view -c bwa_out/Reb1_R1/Reb1_R1.uniq.bam\n1967026\n\n\n\n Hint: Use the Explain SAM flags resource.\nWe can also use fastqc on bam files to look at the quality and statistics for our uniquely mapped reads.\ncat samples.txt | parallel -j 4 fastqc bwa_out/{}/{}.uniq.bam\nmultiqc -o bwa_out bwa_out\n\n\nKey Aims:\n\n\nAlign reads to a reference genome\nUnderstand alignment file formats\nFilter alignments for further analyses"
  }
]